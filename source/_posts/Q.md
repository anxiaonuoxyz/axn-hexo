---
title: Qmix and RODE article read
---
https://www.zhihu.com/question/319565677?sort=created 如何理解强化学习中的折扣率？



# Value-Decomposition Networks For Cooperative Multi-Agent Learning

具有单一联合奖励信号的多智能体协同强化学习问题

## 摘要

基于Value的方法，集中的端到端方式来分散训练

QMix采用一种网络它将联合行动值作为每个agent值的复杂非线性组合进行估算，该组合仅以局部观测为条件。

从结构上强制每个agent的联合行动值是单调的，这允许在非政策学习中实现联合行动值的易于处理的最大化，并保证集中和分散的政策之间的一致性。

## 介绍

要求学习分散政策，分散只考虑每个agent的local的动作观察历史，这样减弱了联合行动空间随智能体数量呈指数增长的问题

分散的政策通常可以在模拟或实验室环境中以集中的方式学习。围绕如何最好地**集中exploit**的许多挑战仍然存在。

如何表示和使用大多数RL方法的action-value函数，一方面适当地捕捉agent行为的影响需要一个集中的action-value函数Qtot，另一方面，agent太多了函数难以学习，分散政策无法允许每个agent只根据个人观察选择一个单独的行动。



确保在$Q_{tot}$执行的一个全局argmax操作与在每个$Q_{a}$执行的一组单独argmax操作产生相同的结果。因此需要施加约束

![image-20210623164309131](https://i.loli.net/2021/06/23/efbSHsl7yKdUYu4.png)

QMIX由代表每个Qa的agent networks和mixing network组成，**混合网络将它们组合成Qtot，而不是VDN中的简单和**。但以一种复杂的非线性方式确保了集中和分散政策之间的一致性。

QMIX可以用因数表示来表示复杂的集中action-value函数，这种因数表示可以很好地扩展agent的数量，并允许通过线性时间的单个argmax操作轻松提取分散策略。

## Related Work

一方面，为多智能体系统寻找策略的自然方法是直接学习分散的价值函数或策略：**Q-learning和VDN**这些方法虽然实现了微不足道的去中心化，但由于同时学习和探索代理所导致的环境的非平稳性，容易产生不稳定性。

另一方面，联合行动的集中学习可以自然地处理协调问题，避免非平稳性，但由于联合行动的空间在代理的数量上呈指数级增长，因此很难进行扩展**：coordination graphs**（该方法通过将一个global奖励函数分解为一组agent-local术语，来挖掘agent之间的条件独立性），**Sparse cooperative Q-learning（**一个表格式的q学习算法，它只学习协调一组合作智能体在必要状态下的行为，并将这些依赖关系编码到一个协调图中）这些方法要求预先提供**agent之间的依赖关系**，而我们不需要这样的先验知识。相反，我们假设每个agent总是对global奖励做出贡献，并了解其在每个state的贡献大小。

一些研究开发了混合方法，利用集中学习和完全分散执行的设置。COMA（[多智能体强化学习笔记04 COMA算法原理介绍 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/127010848)）使用一个**集中的批评者来训练分散的参与者**，估计每个代理的反事实优势函数，以解决多代理的信用分配问题。

VDN（value decomposition networks 价值分解网络），允许集中的value-function学习和分散的执行，将中央state-action值函数分解为单个agent和

QMIX依靠一个神经网络将集中状态转换为另一个神经网络的权重，第二种神经网络通过保持其权值为正来约束其相对于输入是单调的。

## Background

一个完全协作的多智能体任务可以被描述为Dec-POMDP 

![image-20210624175017410](C:\Users\28918\AppData\Roaming\Typora\typora-user-images\image-20210624175017410.png)

![image-20210624175024997](C:\Users\28918\AppData\Roaming\Typora\typora-user-images\image-20210624175024997.png)

![image-20210624175039262](https://i.loli.net/2021/06/24/Rk619hiVX5bcFG2.png)

![image-20210624175059060](https://i.loli.net/2021/06/24/f6nJqHhtTozGvOr.png)

![image-20210624175114835](https://i.loli.net/2021/06/24/2wz4xBl9tgsXDQL.png)

[多智能体强化学习入门（五）——QMIX算法分析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/55003734)



# RODE: LEARNING ROLES TO DECOMPOSE MULTI-AGENT TASKS

[[GitHub - TonghanWang/RODE: Codes accompanying the paper "RODE: Learning Roles to Decompose Multi-Agent Tasks (ICLR 2021, https://arxiv.org/abs/2010.01523). RODE is a scalable role-based multi-agent learning method which effectively discovers roles based on joint action space decomposition according to action effects, establishing a new state of the art on the StarCraft multi-agent benchmark.](https://github.com/TonghanWang/RODE)](https://github.com/TonghanWang/RODE)

https://zhuanlan.zhihu.com/p/389177782 多智能体强化学习2021论文（二）RODE

https://zhuanlan.zhihu.com/p/359567528  强化学习论文阅读笔记：RODE

## 摘要

我们首先提出将联合行动空间分解为**有限的角色行动空间**，根据其对**环境和其他主体的影响**进行**聚类**，学习基于动作效果的角色选择器可以使角色发现变得更容易，我们进一步将行动效应信息整合到角色政策中。

#https:// sites.google.com/view/rode-marl 从视频上看 不同的角色role激励agent在某个方向上探索状态空间

1）Agents first move to the edges (*Role 0*) to avoid being surrounded by the enemies, and 2) Agents alternate between attacking and retracting (*Role 2*) to attract and kill part of the enemies, gaining an advantage in numbers.

1）代理首先移动到边缘（角色0）以避免被敌人包围，2）代理在攻击和缩回（角色2）之间交替，以吸引和杀死部分敌人，在数量中获得优势。

## 介绍

根据动作功能分解联合动作空间（角色发现更容易）。直观地说，**当与其他代理合作时，在特定的观察下，只需要能够实现特定功能的操作子集**。例如，在足球游戏中，没有控球权的玩家只需要在进攻时探索如何移动或冲刺。

在实践中，我们建议首先学习**基于效果的动作表示**，并根据动作对环境和其他主体的影响将其**聚类到角色动作空间**中。然后，利用**可用行动**的影响知识，我们训练一个**角色选择器，**以确定相应的角色观察空间。

该设计形成了一个**双层学习框架**。在**顶层**，**角色选择器在较小的角色空间中以较低的时间分辨率协调角色分配**。在**较低**的层次上，**角色策略是在减少的原始行动观察空间中探索策略**。通过这种方法，将一个多智能体合作问题分解为几个具有更少智能体的短视域学习问题，大大降低了学习复杂度。为了进一步提高子问题的学习效率，我们将**角色策略设置在基于学习效果的动作表征上**，从而提高了角色策略跨动作的泛化性。

## 2 RODE LEARNING FRAMEWORK

我们的思想是学会将一个**多智能体协作任务分解成一组子任务**，每个子任务都有一个**小得多的动作-观察空间**。每个子任务都与一个**角色相关联**，使用**相同角色的代**理通过**共享它们的学习**来**集体学习**解决子任务的角色策略。形式上，我们提出子任务和角色的以下定义。

![image-20210716152337552](C:\Users\28918\AppData\Roaming\Typora\typora-user-images\image-20210716152337552.png)

![image-20210716152319038](https://i.loli.net/2021/07/16/f1AZqKClBJ6VcFm.png)

在我们的定义中，一组给定的角色不仅指定了**任务的因数分解**，还指定了子任务的**策略**。这种**基于角色**的表述是一种有效的动态分组机制，其中**agent按角色分组**。这样做的好处是，我们可以学习**针对角色**而不是针对代理的策略，从而为**代理的数量提供可伸缩性**（分组就减少代理数量了），并**减少目标策略的搜索空间**。在我们的框架中，代理通过在代理之间共享的**角色选择器**$\beta$决定扮演哪个角色。

这里T$\to\varPsi$是一个确定性函数，以局部行动观察历史为条件。

我们的目标是学习一套角色$\varPsi^*$*最大化预期的global return$Q^{\varPsi}(s_t,a_t)$=

![image-20210716153148418](https://i.loli.net/2021/07/16/wlqi7otaKSyxNnb.png)

学习角色涉及两个问题:**学习子任务和角色策略**。为了学习子任务，我们需要学习**角色选择器**$\beta$,并确定**角色动作空间**$A_j$.这两个组成部分分别决定了**角色的观察空间和行动空间**。我们现在介绍学习这些组件的框架

![image-20210716153419961](https://i.loli.net/2021/07/16/bXwNCrmdvncyhZe.png)

### 2.1通过学习动作表征来确定角色动作空间 DETERMINING ROLE ACTION SPACES BY LEARNING ACTION REPRESENTATIONS

分类所有动作为不同角色



限制每个角色的动作空间可以显著减少策略的搜索空间。我们的方法的关键是根据**行动属性来分解行动空间**，从而让角色专注于**具有类似效果的行动，**如在《星际争霸2》中**攻击同种类型**的敌人单位。为此，我们学习可以反映行为对环境和其他主体的影响的行为表征。行为的效果可以通过**诱导奖励和局部观察**的变化来衡量。因此，我们制定了一个学习行为表征的目标，激励包括足够的信息，以便在**已知其他主体的行为和当前的观察时**，可以预测下一个观察和奖励。

图中a为动作编码器$f_e$，将ont-hot动作**映射**到d维表示空间。我们用$z_a$表示这个空间中的**动作a**.$z_a$=$f_e(a;\theta_e)$其中$\theta$为参数。它被用来预测下一个观察$o_i^{'}$和全局奖励r，给定一个anget i当前的观测值$o_i$,和其他agent的one-hot动作$a_{-i}$。这个模型可以解释为一个forward model,，它通过最小化下面的损失函数来训练

![image-20210716164605980](https://i.loli.net/2021/07/16/tEkLdoi325v19hf.png)

一开始，我们初始化K个角色，每个角色都有完整的行动空间。在收集样本并训练预测模型的$t_e$时间步长后，**根据动作的潜在表现对动作进行聚类**，并设置**每个角色的动作空间以包含一个聚类**。将离群值添加到所有集群中，以避免只包含一个操作的操作集。更新后，训练开始，每个角色的动作表现和动作空间在训练期间保持不变。

## 2.2学习角色选择器和角色策略 LEARNING THE ROLE SELECTOR AND ROLE POLICIES

根据动作的效果学习动作表示来聚类动作会导致动作空间的因式分解，其中**每个动作子集都可以实现特定的功能**。为了充分利用这种因子分解，我们使用了一个双层层次结构来协调角色和基本动作的选择。在顶层，角色选择器在**每个c时间步中为每个代理分配一个角色**。角色分配完成后，agent在相应的受限角色动作空间中**探索角色策略**。

对于**角色选择器**，我们可以简单地使用**传统的Q-network**，它的**输入**是局部动作观察历史，**输出**是每个角色的q值。但是，这种结构可能并不有效，因为它**忽略**了不同角色的**行动空间信息**。从直观上看，**选择角色就是选择下一个c时间步要执行的动作子集**，所以**选择角色的q值与其角色动作空间密切相关**。因此，我们建议**基于可用动作的平均表示来选择角色**。图1(b)显示了提议的**角色选择器**结构。具体来说,我们称之为

![image-20210718112234599](https://i.loli.net/2021/07/18/NjhKQagRV9Ftsen.png)

$\rho_j$**为角色的表征，$A_j$是其受限的动作空间，为了选择角色，agent共享一个线性层和一个参数为$\theta_{\tau\beta}$的GRU，编码本地动作观察历史$\tau$为固定长度的向量$h_{\tau}$.一个参数为$\theta_{\beta}$的全连接层$f_{\beta}(h_{\tau};\theta_{\beta})$作为角色选择器，然后将$h_{\tau}$映射到$z_{\tau}$,并估算agent i选择**角色$\rho_j$的预期收益为

![image-20210718115552355](https://i.loli.net/2021/07/18/foFsQpyT24jAvg5.png)

代理同时选择它们的角色，这可能会导致意外的行为。例如，在《星际争霸2》中，所有选择攻击特定单位类型的特工都会造成过度杀伤（**打一个伤害溢出**）。为了更好地协调角色分配，我们使用mixing network来估计**全局q值**。所以**角色选择器可以使用全局奖励进行训练**。本文使用**QMIX**引入的混合网络进行单调逼近。然后我们将以下td loss最小化，**以更新角色选择器**

![image-20210718120752326](https://i.loli.net/2021/07/18/3T1RMwCdmQHzAn8.png)

角色分配完成后，在接下来的c个时间步中，**agent只能执行对应角色动作空间中的操作**。与角色选择器类似，学习角色策略最直接的方法是使用传统的**深度q -network** ，它直接估计每个动作的q值。而基于动作表征的q值充分利用了动作的效果信息，可以更好地在动作间泛化。

因此，我们使用图1(c)所示的框架来学习**角色策略**。具体来说，**代理再次使用一个共享的线性层和一个GRU**，**它们一起编码了本地的行动观察历史$\tau$为向量$h_\tau$.**

![image-20210718121752888](https://i.loli.net/2021/07/18/4dXtF9cwMpkzUJS.png)

并且**使用来自与角色选择器相同的重放缓冲区D的统一样本估计期望**,此外，仅在训练过程中使用角色选择器和角色策略两个混合网络进行训练。由于基于效果的动作表示，我们的框架在实践中是轻量级的，我们使用一个没有激活函数的单一线性层作为角色策略，使用一个两层全连接的网络作为角色选择器。

## 3 RELATED WORK

1RODE采用了分层决策结构 hierarchical decisionmaking structure.

2使用动作表示来分解多智能体任务。 action representations

3其中通过基于效应 的行动空间分解(effect-based action space factorization )和基于角色的层次结构( role-based hierarchical structure)来**减少搜索空间**，该结构设计用于在这些因素行动空间中进行高效学习

## 4 EXPERIMENTS

我们设计实验来回答以下问题:

(1)所提出的预测模型能否学习反映行为影响的行为表征?

(2)RODE能提高学习效率吗?如果是，那么哪个组件对性能提高贡献最大

(3)rode可以支持快速转移到具有不同行动和代理的环境的环境吗？

(4)角色选择者能否学习可解释的高层合作行为

sc2 四个方向移动 stop noop 为基础 有几个敌人就有几个攻击敌人的动作 **总共是n+6个动作**

### 4.1动作表征

corridor map 6xx打24狗 特点是是同质的代理人和敌人。在这个任务中，所有的攻击行动都具有相似的效果，因为敌人是同质的。此外，由于地图的对称性，向北和向东移动也会对环境产生类似的影响，使agent朝向敌人。同样地，向南和向西移动也会产生同样的效果，因为这两种行动都会让代理人远离敌人。习得的动作编码器捕获动作空间中的这些底层结构(图2左)。**我们可以在潜在动作表征空间中观察到三个清晰的簇，分别对应于上述三种类型的动作**

3s5z vs 3s6z map。敌人团队由不同的单位组成，攻击潜行者和狂热者有不同的效果。我们的预测模型强调了这一差异，攻击行为分为两组(图2右)。此外，习得的行动编码器还捕捉到向北/向南移动和向其他方向移动之间的差异，这两种移动行动不会让agent靠近或远离敌人。

![image-20210718135639841](https://i.loli.net/2021/07/18/pGIPLmQoVqsN92b.png)

### 4.2 PERFORMANCE AND ABLATION STUDY 性能和消融研究

难度很高，复杂的地图更牛逼，相比之下，在5张简单的地图上，RODE通常需要更多的样本来学习一个成功的策略。

![image-20210718135948364](https://i.loli.net/2021/07/18/fECMwzasnqBWybm.png)

![image-20210718140159230](https://i.loli.net/2021/07/18/qy6iD1ApHxwalgf.png)



为了了解RODE的优越性能，我们开展了消融研究，以测试其四个主要组成部分的贡献:(A)受限的角色作用空间;(B)利用动作效果来集中行动;(C)将动作表征集成到角色政策和角色选择器中;(D)分层学习结构。为了测试组件A和组件B，在50K样本后更新角色动作空间时，我们让每个角色动作空间分别包含所有动作或动作的一个随机子集，而框架中的其他部分保持不变。对于组件B，我们确保角色动作空间的并集等于整个动作空间。为了测试组件C，我们可以简单地使用传统的q -网络来学习角色策略和角色选择器。组件D的测试有一点不同，因为我们不能在使用不同角色的同时单独取消它。因此，我们通过使用传统的深度q网络(用于角色策略和角色选择器)去除组件A和C来测试它，并允许角色策略从所有原始动作中进行选择。

![image-20210718141340781](C:\Users\28918\AppData\Roaming\Typora\typora-user-images\image-20210718141340781.png)



### 4.3 POLICY TRANSFER 政策转移

我们可以将学习到的策略转移到具有与**旧动作相似效果的新动作**的任务中**。由于角色动作空间仍然期望包含具有类似效果的动作，这可以通过旧的角色表示来反映**，所以我们不重新计算新任务上的角色表示。此外，在我们的框架中，**是角色与环境交互，代理只需要分配角色**。因此，我们学习到的策略也可以转移到具有不同代理数的任务中

![image-20210718155344262](C:\Users\28918\AppData\Roaming\Typora\typora-user-images\image-20210718155344262.png)

使用不同数目的代理 证明RODE的迁移性



## 5附录

### A 架构、超参数和基础设施

在本文中，我们使用简单的网络结构来实现角色选择器和角色策略。每个**角色策略都是一个简单的线性网络**，没有隐藏层或激活函数，**角色选择器是一个具有64维隐藏层的两层前馈全连接网络**。agent共享两层轨迹编码网络**，一层为全连接层，另一层为64维隐藏状态的GRU层**。对于所有实验，**动作表示的长度d**被设置为20。角色策略的输出和角色选择器的输出分别被送**入各自独立的qmix风格的混合**网络，以估计全局动作价值。这两个混合网络使用相同的架构，包含一个32维的ReLU激活隐藏层。混合网络的参数是由超网络对全局状态的调节产生的。这些设置与QMIX相同

### B案例研究:角色动力学 CASE STUDY: ROLE DYNAMICS

**为什么限制角色的行动空间可以改善学习性能**

我们从SMAC基准出发对地图走廊进行了研究。我们选择这张地图是因为它呈现了一个艰难的探索任务。这种情况下的最佳策略是，盟军的狂热者需要进行积极的**状态空间探索**，以便学会移动到地图的边缘或阻塞点，以避免被敌军包围。之前最先进的算法都学习了一种**次优**策略，即代理只是为了获得奖励而伤害敌人单位，而不是先移动到边缘然后进行攻击。

![image-20210718160832698](https://i.loli.net/2021/07/18/I1UtPXrwoCKphba.png)

我们看到所有的agent在一开始都选择了Role 2(图7 (a))来帮助它们**移动到边缘**。代理也学习了一个有效的合作策略，其中一个代理(agnet  A)吸引了大部分的敌人(敌人组A)，这样它的队友在面对较少的敌人(敌人组B)时就有优势。杀死agnet A后，敌方A组通过咽喉到达目的地。在图7 (b)中，活着的agnet 开始攻击这群敌人。然而，敌人仍然非常强大。为了赢得游戏，盟友agnet 学会吸引并杀死部分敌人。它们通过在Role 0(在本例中是撤退)和Role 1(攻击)之间交替实现这一点，如图7 (b-d)所示。

why？我们基于效果的行动聚类将完整的行动空间分解成**更小的角色行动空间**，每个角色行动空间专注于**具有类似效果的行动的更小子集**。由于每个角色都在一个有相似动作的更小的行动空间中学习，这使得agent能够**有效地探索**，并导致每个角色能够学习不同的策略，比如扩散和收缩。此外，在更小的角色空间中进行探索，使得这些**子策略的协调**更加容易。

**在分层分解的动作空间中进行探索也会对探索空间产生偏见**。例如，在corridor 地图上，Role 0和Role 2会激励agent朝着某个方向探索状态空间，从而帮助他们学习这张地图上的几个重要策略:1)代理首先移动到边缘(Role 0)以避免被敌人包围(图7 (a))， 2)代理交替攻击(Role 1)和撤退(Role 2)以吸引并杀死部分敌人，获得数量优势(图7 (b-d))。图8也支持角色0和角色2的帮助探索频率随着主体探索次数的增加而先提高，然后随着合作策略的逐渐收敛而降低。

在其他地图上，受限的角色动作空间可以帮助代理以其他方式解决任务。例如，在地图3s5z vs 3s6z中，代理学习效率更高，因为动作编码器将任务分解成**两个子任务**，一个专注于攻击狂热者，另一个专注于攻击潜行者。这些受限子问题的学习策略显著降低了学习复杂度。总之，**动作效应为联合动作空间的分解提供了有效的信息**。根据动作的效果进行聚类可以使角色发现更加容易，在较小的动作空间中学习合作策略可以使学习更加容易处理。

### C实验细节

### D角色间隔和循环角色选择器

#### D.1 ROLE INTERVAL

在我们的框架中，**角色选择器在每个c时间步中将角色分配给每个代理**。我们称c为角色间隔。角色间隔决定动作空间更改的频率，并可能对性能产生关键影响。为了更好地了解这种影响，我们更改角色间隔，同时保持其他部分不变，并在多个环境上测试RODE。

![image-20210718180238124](https://i.loli.net/2021/07/18/DshdFKH6JzB3u9r.png)

#### D.2循环角色选择器和全连接角色选择器

角色选择器应该学习观察空间的分解。然而，我们基于本地的行动观察历史来构建这个组件。在本节中，我们将讨论在角色选择器中使用GRU的影响。在图13中，我们比较了以局部观察为条件的全连接角色选择器的RODE和RODE。正如预期的那样，**基于本地观察的循环角色选择器比选择器执行得更好。**

### E聚类算法

#### E.1 k值的影响

一般来说，k值为3或5可以很好地适用于这些地图。实际上，我们将k = 3设置在具有同种敌人的地图上，而将k = 5设置在具有同种敌人的地图上。

为了进一步理解不同的k值是如何影响学习性能的，我们以一个超级困难的地图6h vs 8z为例。当k=3或5时，聚类方法(在向所有其他聚类添加离群值之后)给出三个角色动作空间 Move Eastward & Northward, Attack, and Move Westward & Southward.。类似于附录B中讨论的走廊地图的情况，这些角色可以改善探索，从而提高性能。然而，当k=7时，**所有的移动动作都成为离群值。结果是大部分角色动作空间变成了原来的完整动作空间。**这种对动作空间的分解不会减少搜索空间，因此性能接近于QMIX。

![image-20210718181414590](https://i.loli.net/2021/07/18/PUAgY1z2sDKhpG9.png)

为了避免k值的选择和异常值的处理方式之间的这种复杂交互，我们建议在聚类动作表示时使用X-means，我们将在下一节详细讨论

#### E.2其他聚类算法

X-means (Pelleg等人，2000)通过**自动确定簇的数量**改进了k-means。我们使用最小的无噪声描述长度作为X-means 的分裂标准，并在所有14张图上使用X-means测试RODE。结果如图15所示。我们可以看到，结果与预先定义的k值的k-means相当。因此，当面对一个新任务时，我们推荐X-means，而对于用户知道角色数量的任务，k-means可以更准确。

<img src="https://i.loli.net/2021/07/18/JPqUCjy7m4zfeRh.png" alt="image-20210718181151029" style="zoom:150%;" />

### F ROLE REPRESENTATIONS 角色表示

![image-20210718181537930](https://i.loli.net/2021/07/18/nyUdXcoxT9tw2r5.png)

我们使用角色动作空间中的**动作的平均**表示来表示一个角色。我们发现，**角色选择器和角色策略的Q网络(包括处理本地观测的gru和网络($f_\beta$或$f_{\rho_j}$)的分离**对于确保这些角色表示具有足够的表达性是重要的。
